{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOIYt+KPodq0+db4yT7oExz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarvAster/Graphs-GNNs-KGs/blob/main/Node_Prop_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZNgv5kUDotv",
        "outputId": "be108a1e-133c-4771-efa2-831c4ad03b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch has version 2.5.1+cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.5.1+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_scatter-2.1.2%2Bpt25cu121-cp310-cp310-linux_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt25cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.5.1+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_sparse-0.6.18%2Bpt25cu121-cp310-cp310-linux_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt25cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.6)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.2.3)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (75.1.0)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.6.0->ogb) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2024.8.30)\n",
            "Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.4 ogb-1.3.6 outdated-0.2.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/ENZYMES.zip\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENZYMES(600)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "print(\"PyTorch has version {}\".format(torch.__version__))\n",
        "torch_version = str(torch.__version__)\n",
        "scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "!pip install torch-scatter -f $scatter_src\n",
        "!pip install torch-sparse -f $sparse_src\n",
        "!pip install torch-geometric\n",
        "!pip install ogb\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "root = './enzymes'\n",
        "name = 'ENZYMES'\n",
        "\n",
        "# The ENZYMES dataset\n",
        "pyg_dataset= TUDataset(root, name)\n",
        "\n",
        "# You will find that there are 600 graphs in this dataset\n",
        "print(pyg_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_num_classes(pyg_dataset):\n",
        "  return pyg_dataset.num_classes\n",
        "\n",
        "def get_num_features(pyg_dataset):\n",
        "  num_features = pyg_dataset.num_features\n",
        "  return num_features\n",
        "\n",
        "num_classes = get_num_classes(pyg_dataset)\n",
        "num_features = get_num_features(pyg_dataset)\n",
        "print(\"{} dataset has {} classes\".format(name, num_classes))\n",
        "print(\"{} dataset has {} features\".format(name, num_features))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKEJkdA0Em3O",
        "outputId": "116e56f7-7749-4fd9-ba43-3ff4a545841e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENZYMES dataset has 6 classes\n",
            "ENZYMES dataset has 3 features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_graph_class(pyg_dataset, idx):\n",
        "  label = pyg_dataset[idx].y.item()\n",
        "  return label\n",
        "\n",
        "# Here pyg_dataset is a dataset for graph classification\n",
        "\n",
        "graph_0 = pyg_dataset[0]\n",
        "print(graph_0)\n",
        "print(pyg_dataset[0].y.item())\n",
        "idx = 100\n",
        "label = get_graph_class(pyg_dataset, idx)\n",
        "print('Graph with index {} has label {}'.format(idx, label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ry_viI_VNbEa",
        "outputId": "2141d7c9-7930-44ad-e092-0355df8b0f5e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(edge_index=[2, 168], x=[37, 3], y=[1])\n",
            "5\n",
            "Graph with index 100 has label 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_graph_num_edges(pyg_dataset, idx):\n",
        "  data = pyg_dataset[idx]\n",
        "  edges = set()\n",
        "  for i in range(data.edge_index.shape[1]):\n",
        "    u,v = (data.edge_index[0,i].item(), data.edge_index[1,i].item())\n",
        "    edges.add((max(u,v), min(u,v)))\n",
        "  return len(edges)\n",
        "\n",
        "idx = 200\n",
        "num_edges = get_graph_num_edges(pyg_dataset, idx)\n",
        "print('Graph with index {} has {} edges'.format(idx, num_edges))\n",
        "print(pyg_dataset[idx].num_edges)"
      ],
      "metadata": {
        "id": "hV7bXm4XQVt8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a991956-c2ba-4ea7-b35e-8e3fce6e3d6f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph with index 200 has 53 edges\n",
            "106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "\n",
        "dataset_name = 'ogbn-arxiv'\n",
        "# Load the dataset and transform it to sparse tensor\n",
        "dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                transform=T.ToSparseTensor())\n",
        "print('The {} dataset has {} graph'.format(dataset_name, len(dataset)))\n",
        "\n",
        "# Extract the graph\n",
        "data = dataset[0]\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUVdPH9mDQdN",
        "outputId": "71165950-ad05-41d4-8575-09977e2c19b9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.08 GB: 100%|██████████| 81/81 [00:10<00:00,  7.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2074.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2565.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Done!\n",
            "/usr/local/lib/python3.10/dist-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data, self.slices = torch.load(self.processed_paths[0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ogbn-arxiv dataset has 1 graph\n",
            "Data(num_nodes=169343, x=[169343, 128], node_year=[169343, 1], y=[169343, 1], adj_t=[169343, 169343, nnz=1166243])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def graph_num_features(data):\n",
        "  return data.num_features\n",
        "\n",
        "num_features = graph_num_features(data)\n",
        "print('The graph has {} features'.format(num_features))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seA9p00EEoJK",
        "outputId": "f73d079c-101d-4e28-b08c-5f13cc5693a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The graph has 128 features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "print(torch.__version__)\n",
        "\n",
        "# The PyG built-in GCNConv\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIcIAwHnFBcG",
        "outputId": "51855af9-f193-4381-c3c7-1bb8ecbea041"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = 'ogbn-arxiv'\n",
        "dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                transform=T.ToSparseTensor())\n",
        "data = dataset[0]\n",
        "\n",
        "# Make the adjacency matrix to symmetric\n",
        "data.adj_t = data.adj_t.to_symmetric()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# If you use GPU, the device should be cuda\n",
        "print('Device: {}'.format(device))\n",
        "\n",
        "data = data.to(device)\n",
        "split_idx = dataset.get_idx_split()\n",
        "train_idx = split_idx['train'].to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymet-FBoFHGD",
        "outputId": "5e7153d1-8eaa-47a5-cd35-e857474a1dd8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data, self.slices = torch.load(self.processed_paths[0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
        "                 dropout, return_embeds=False):\n",
        "\n",
        "        super(GCN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        # A list of GCNConv layers\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "\n",
        "        # A list of 1D batch normalization layers\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        self.convs.append(GCNConv(input_dim,hidden_dim))\n",
        "        for _ in range(self.num_layers-1):\n",
        "            self.convs.append(GCNConv(hidden_dim,hidden_dim))\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "        self.convs.append(GCNConv(hidden_dim,output_dim))\n",
        "\n",
        "        # The log softmax layer\n",
        "        self.softmax = torch.nn.LogSoftmax()\n",
        "\n",
        "        ## The parameters you can set for GCNConv include 'in_channels' and\n",
        "        ## 'out_channels'. For more information please refer to the documentation:\n",
        "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv\n",
        "        ## The only parameter you need to set for BatchNorm1d is 'num_features'\n",
        "        ## https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)\n",
        "\n",
        "        # Probability of an element getting zeroed\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Skip classification layer and return node embeddings\n",
        "        self.return_embeds = return_embeds\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        for i in range(self.num_layers-1):\n",
        "          x= self.convs[i](x,adj_t)\n",
        "          x = self.bns[i](x)\n",
        "          x = torch.nn.functional.relu(x)\n",
        "          x = torch.nn.functional.dropout(x,self.dropout)\n",
        "\n",
        "        x = self.convs[-1](x,adj_t)\n",
        "        if self.return_embeds:\n",
        "          return x\n",
        "        else:\n",
        "          return self.softmax(x)"
      ],
      "metadata": {
        "id": "3ko2YotqGAxl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data, train_idx, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.adj_t)\n",
        "    loss = loss_fn(out[train_idx], data.y.squeeze(1)[train_idx])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "TzBu9C1NnA31"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test function here\n",
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator, save_model_results=False):\n",
        "    # TODO: Implement a function that tests the model by\n",
        "    # using the given split_idx and evaluator.\n",
        "    model.eval()\n",
        "\n",
        "    # The output of model on all data\n",
        "    out = model(data.x, data.adj_t)\n",
        "\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    if save_model_results:\n",
        "      print (\"Saving Model Predictions\")\n",
        "\n",
        "      data = {}\n",
        "      data['y_pred'] = y_pred.view(-1).cpu().detach().numpy()\n",
        "\n",
        "      df = pd.DataFrame(data=data)\n",
        "      # Save locally as csv\n",
        "      df.to_csv('ogbn-arxiv_node.csv', sep=',', index=False)\n",
        "\n",
        "\n",
        "    return train_acc, valid_acc, test_acc"
      ],
      "metadata": {
        "id": "-ofjx1ogvZlO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "  'device': device,\n",
        "  'num_layers': 5,\n",
        "  'hidden_dim': 512,\n",
        "  'dropout': 0.3,\n",
        "  'lr': 0.005,\n",
        "  'epochs': 100,\n",
        "}\n",
        "args\n",
        "model = GCN(data.num_features, args['hidden_dim'],\n",
        "            dataset.num_classes, args['num_layers'],\n",
        "            args['dropout']).to(device)\n",
        "evaluator = Evaluator(name='ogbn-arxiv')"
      ],
      "metadata": {
        "id": "4eiuLwomn8Af"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training should take <10min using GPU runtime\n",
        "import copy\n",
        "# reset the parameters to initial random value\n",
        "model.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "loss_fn = F.nll_loss\n",
        "\n",
        "best_model = None\n",
        "best_valid_acc = 0\n",
        "\n",
        "for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "  loss = train(model, data, train_idx, optimizer, loss_fn)\n",
        "  result = test(model, data, split_idx, evaluator)\n",
        "  train_acc, valid_acc, test_acc = result\n",
        "  if valid_acc > best_valid_acc:\n",
        "      best_valid_acc = valid_acc\n",
        "      best_model = copy.deepcopy(model)\n",
        "  print(f'Epoch: {epoch:02d}, '\n",
        "        f'Loss: {loss:.4f}, '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ],
      "metadata": {
        "id": "bm3oTAzHoGw0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51c018c7-5aa7-46f0-ffc0-4583e5d98d97"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01, Loss: 4.3006, Train: 25.13%, Valid: 29.23% Test: 26.23%\n",
            "Epoch: 02, Loss: 2.1855, Train: 20.34%, Valid: 23.82% Test: 30.57%\n",
            "Epoch: 03, Loss: 2.8909, Train: 26.86%, Valid: 30.13% Test: 34.70%\n",
            "Epoch: 04, Loss: 1.9346, Train: 22.85%, Valid: 22.84% Test: 27.61%\n",
            "Epoch: 05, Loss: 1.6702, Train: 24.34%, Valid: 22.13% Test: 26.88%\n",
            "Epoch: 06, Loss: 1.6081, Train: 30.24%, Valid: 25.30% Test: 29.38%\n",
            "Epoch: 07, Loss: 1.5378, Train: 34.25%, Valid: 26.03% Test: 28.03%\n",
            "Epoch: 08, Loss: 1.4375, Train: 32.06%, Valid: 20.66% Test: 18.76%\n",
            "Epoch: 09, Loss: 1.3841, Train: 29.09%, Valid: 16.41% Test: 13.61%\n",
            "Epoch: 10, Loss: 1.3577, Train: 27.32%, Valid: 15.16% Test: 12.33%\n",
            "Epoch: 11, Loss: 1.3337, Train: 27.19%, Valid: 15.41% Test: 12.62%\n",
            "Epoch: 12, Loss: 1.3063, Train: 29.03%, Valid: 16.56% Test: 13.60%\n",
            "Epoch: 13, Loss: 1.2794, Train: 31.30%, Valid: 18.23% Test: 14.99%\n",
            "Epoch: 14, Loss: 1.2488, Train: 32.77%, Valid: 19.59% Test: 16.75%\n",
            "Epoch: 15, Loss: 1.2288, Train: 33.16%, Valid: 21.72% Test: 20.09%\n",
            "Epoch: 16, Loss: 1.2119, Train: 34.06%, Valid: 23.96% Test: 24.33%\n",
            "Epoch: 17, Loss: 1.1951, Train: 35.10%, Valid: 24.99% Test: 25.49%\n",
            "Epoch: 18, Loss: 1.1795, Train: 35.22%, Valid: 25.22% Test: 25.18%\n",
            "Epoch: 19, Loss: 1.1627, Train: 35.35%, Valid: 25.74% Test: 25.50%\n",
            "Epoch: 20, Loss: 1.1462, Train: 35.13%, Valid: 24.84% Test: 24.21%\n",
            "Epoch: 21, Loss: 1.1317, Train: 34.87%, Valid: 24.09% Test: 24.55%\n",
            "Epoch: 22, Loss: 1.1246, Train: 35.16%, Valid: 25.12% Test: 26.27%\n",
            "Epoch: 23, Loss: 1.1160, Train: 36.60%, Valid: 27.08% Test: 29.12%\n",
            "Epoch: 24, Loss: 1.1015, Train: 39.37%, Valid: 29.44% Test: 31.44%\n",
            "Epoch: 25, Loss: 1.0904, Train: 42.38%, Valid: 32.86% Test: 34.86%\n",
            "Epoch: 26, Loss: 1.0782, Train: 45.35%, Valid: 35.98% Test: 37.88%\n",
            "Epoch: 27, Loss: 1.0686, Train: 48.84%, Valid: 41.10% Test: 43.48%\n",
            "Epoch: 28, Loss: 1.0596, Train: 51.21%, Valid: 43.74% Test: 46.15%\n",
            "Epoch: 29, Loss: 1.0557, Train: 52.56%, Valid: 44.90% Test: 47.06%\n",
            "Epoch: 30, Loss: 1.0481, Train: 53.41%, Valid: 44.62% Test: 46.38%\n",
            "Epoch: 31, Loss: 1.0366, Train: 54.19%, Valid: 45.48% Test: 46.74%\n",
            "Epoch: 32, Loss: 1.0268, Train: 55.46%, Valid: 47.16% Test: 47.94%\n",
            "Epoch: 33, Loss: 1.0240, Train: 57.68%, Valid: 51.78% Test: 51.74%\n",
            "Epoch: 34, Loss: 1.0161, Train: 59.06%, Valid: 54.25% Test: 53.86%\n",
            "Epoch: 35, Loss: 1.0126, Train: 60.51%, Valid: 56.10% Test: 55.34%\n",
            "Epoch: 36, Loss: 1.0048, Train: 60.91%, Valid: 56.88% Test: 55.83%\n",
            "Epoch: 37, Loss: 0.9992, Train: 61.50%, Valid: 58.38% Test: 57.51%\n",
            "Epoch: 38, Loss: 0.9934, Train: 62.26%, Valid: 60.13% Test: 58.68%\n",
            "Epoch: 39, Loss: 0.9886, Train: 62.83%, Valid: 61.17% Test: 59.28%\n",
            "Epoch: 40, Loss: 0.9836, Train: 63.75%, Valid: 62.38% Test: 60.63%\n",
            "Epoch: 41, Loss: 0.9789, Train: 64.85%, Valid: 63.12% Test: 61.78%\n",
            "Epoch: 42, Loss: 0.9727, Train: 65.46%, Valid: 63.66% Test: 62.19%\n",
            "Epoch: 43, Loss: 0.9696, Train: 66.38%, Valid: 64.49% Test: 63.44%\n",
            "Epoch: 44, Loss: 0.9620, Train: 67.08%, Valid: 66.15% Test: 64.86%\n",
            "Epoch: 45, Loss: 0.9615, Train: 67.27%, Valid: 65.68% Test: 64.43%\n",
            "Epoch: 46, Loss: 0.9577, Train: 67.32%, Valid: 65.16% Test: 63.62%\n",
            "Epoch: 47, Loss: 0.9525, Train: 68.01%, Valid: 65.41% Test: 63.86%\n",
            "Epoch: 48, Loss: 0.9472, Train: 68.57%, Valid: 66.42% Test: 65.23%\n",
            "Epoch: 49, Loss: 0.9426, Train: 68.96%, Valid: 66.84% Test: 65.72%\n",
            "Epoch: 50, Loss: 0.9392, Train: 69.54%, Valid: 67.30% Test: 66.27%\n",
            "Epoch: 51, Loss: 0.9375, Train: 69.59%, Valid: 67.30% Test: 65.92%\n",
            "Epoch: 52, Loss: 0.9345, Train: 70.25%, Valid: 68.07% Test: 66.53%\n",
            "Epoch: 53, Loss: 0.9286, Train: 70.36%, Valid: 68.08% Test: 66.73%\n",
            "Epoch: 54, Loss: 0.9249, Train: 70.71%, Valid: 68.35% Test: 67.33%\n",
            "Epoch: 55, Loss: 0.9221, Train: 70.46%, Valid: 68.33% Test: 67.12%\n",
            "Epoch: 56, Loss: 0.9201, Train: 70.44%, Valid: 68.27% Test: 66.45%\n",
            "Epoch: 57, Loss: 0.9148, Train: 70.65%, Valid: 68.22% Test: 66.92%\n",
            "Epoch: 58, Loss: 0.9109, Train: 71.31%, Valid: 68.66% Test: 67.58%\n",
            "Epoch: 59, Loss: 0.9091, Train: 71.46%, Valid: 69.43% Test: 67.90%\n",
            "Epoch: 60, Loss: 0.9049, Train: 71.71%, Valid: 69.75% Test: 68.73%\n",
            "Epoch: 61, Loss: 0.9027, Train: 71.67%, Valid: 68.97% Test: 67.77%\n",
            "Epoch: 62, Loss: 0.9008, Train: 71.74%, Valid: 69.14% Test: 67.86%\n",
            "Epoch: 63, Loss: 0.8945, Train: 71.59%, Valid: 69.14% Test: 67.61%\n",
            "Epoch: 64, Loss: 0.8956, Train: 71.44%, Valid: 68.79% Test: 67.27%\n",
            "Epoch: 65, Loss: 0.8914, Train: 71.52%, Valid: 68.78% Test: 66.91%\n",
            "Epoch: 66, Loss: 0.8865, Train: 71.60%, Valid: 68.87% Test: 67.32%\n",
            "Epoch: 67, Loss: 0.8864, Train: 71.56%, Valid: 69.37% Test: 68.15%\n",
            "Epoch: 68, Loss: 0.8828, Train: 71.66%, Valid: 69.24% Test: 68.02%\n",
            "Epoch: 69, Loss: 0.8790, Train: 71.59%, Valid: 68.77% Test: 67.78%\n",
            "Epoch: 70, Loss: 0.8793, Train: 72.05%, Valid: 70.02% Test: 69.02%\n",
            "Epoch: 71, Loss: 0.8736, Train: 72.23%, Valid: 70.05% Test: 69.22%\n",
            "Epoch: 72, Loss: 0.8700, Train: 71.78%, Valid: 69.64% Test: 68.74%\n",
            "Epoch: 73, Loss: 0.8696, Train: 72.06%, Valid: 68.91% Test: 67.47%\n",
            "Epoch: 74, Loss: 0.8671, Train: 72.28%, Valid: 69.44% Test: 68.23%\n",
            "Epoch: 75, Loss: 0.8627, Train: 72.39%, Valid: 70.24% Test: 69.31%\n",
            "Epoch: 76, Loss: 0.8604, Train: 72.52%, Valid: 70.32% Test: 69.55%\n",
            "Epoch: 77, Loss: 0.8570, Train: 72.27%, Valid: 69.35% Test: 67.79%\n",
            "Epoch: 78, Loss: 0.8578, Train: 72.69%, Valid: 69.47% Test: 67.70%\n",
            "Epoch: 79, Loss: 0.8546, Train: 72.66%, Valid: 69.68% Test: 68.38%\n",
            "Epoch: 80, Loss: 0.8503, Train: 72.81%, Valid: 69.64% Test: 68.13%\n",
            "Epoch: 81, Loss: 0.8478, Train: 73.21%, Valid: 69.77% Test: 68.04%\n",
            "Epoch: 82, Loss: 0.8458, Train: 73.08%, Valid: 70.66% Test: 69.66%\n",
            "Epoch: 83, Loss: 0.8451, Train: 72.68%, Valid: 69.02% Test: 68.00%\n",
            "Epoch: 84, Loss: 0.8413, Train: 72.85%, Valid: 69.82% Test: 68.47%\n",
            "Epoch: 85, Loss: 0.8387, Train: 72.38%, Valid: 69.53% Test: 68.69%\n",
            "Epoch: 86, Loss: 0.8395, Train: 72.62%, Valid: 68.74% Test: 66.90%\n",
            "Epoch: 87, Loss: 0.8348, Train: 72.05%, Valid: 67.79% Test: 65.96%\n",
            "Epoch: 88, Loss: 0.8324, Train: 72.66%, Valid: 69.14% Test: 67.44%\n",
            "Epoch: 89, Loss: 0.8301, Train: 72.94%, Valid: 69.15% Test: 67.37%\n",
            "Epoch: 90, Loss: 0.8300, Train: 72.95%, Valid: 70.12% Test: 68.72%\n",
            "Epoch: 91, Loss: 0.8257, Train: 73.45%, Valid: 70.70% Test: 69.60%\n",
            "Epoch: 92, Loss: 0.8223, Train: 73.67%, Valid: 70.25% Test: 68.66%\n",
            "Epoch: 93, Loss: 0.8212, Train: 73.41%, Valid: 70.49% Test: 69.28%\n",
            "Epoch: 94, Loss: 0.8194, Train: 73.58%, Valid: 70.04% Test: 68.80%\n",
            "Epoch: 95, Loss: 0.8182, Train: 73.97%, Valid: 71.34% Test: 70.51%\n",
            "Epoch: 96, Loss: 0.8162, Train: 73.62%, Valid: 70.29% Test: 69.10%\n",
            "Epoch: 97, Loss: 0.8131, Train: 73.74%, Valid: 71.10% Test: 69.85%\n",
            "Epoch: 98, Loss: 0.8123, Train: 73.86%, Valid: 70.24% Test: 69.25%\n",
            "Epoch: 99, Loss: 0.8103, Train: 73.81%, Valid: 69.68% Test: 67.22%\n",
            "Epoch: 100, Loss: 0.8111, Train: 72.44%, Valid: 69.13% Test: 68.11%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  best_result = test(best_model, data, split_idx, evaluator, save_model_results=True)\n",
        "  train_acc, valid_acc, test_acc = best_result\n",
        "  print(f'Best model: '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ],
      "metadata": {
        "id": "sJMFbdY1oZx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6102d1f-eaa4-4882-b9de-ca9717877410"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Model Predictions\n",
            "Best model: Train: 74.05%, Valid: 71.60% Test: 70.75%\n"
          ]
        }
      ]
    }
  ]
}